# ğŸš€ GHID IMPLEMENTARE MIGRARE BIGQUERY

**Data**: 01.10.2025 (ora RomÃ¢niei)
**Obiectiv**: Migrare completÄƒ BigQuery cu partitioning + clustering
**Reducere costuri estimate**: 90-95%

---

## ğŸ“‹ PREREQUISITE

### **1. Instalare Google Cloud SDK È™i bq CLI**

#### **Pe Linux (Ubuntu/Debian)**

**IMPORTANT**: RuleazÄƒ TOATE comenzile Ã®n ordine, nu doar ultima!

```bash
# PASUL 1: InstaleazÄƒ dependinÈ›e
sudo apt-get install apt-transport-https ca-certificates gnupg curl

# PASUL 2: AdaugÄƒ cheia GPG Google Cloud
curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg

# PASUL 3: AdaugÄƒ repository Google Cloud SDK
echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list

# PASUL 4: Update package list È™i instaleazÄƒ
sudo apt-get update && sudo apt-get install google-cloud-sdk

# PASUL 5: VerificÄƒ instalarea
bq version
gcloud version
```

**SAU comandÄƒ all-in-one (copiazÄƒ È™i ruleazÄƒ tot deodatÄƒ):**
```bash
sudo apt-get install -y apt-transport-https ca-certificates gnupg curl && \
curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg && \
echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \
sudo apt-get update && \
sudo apt-get install -y google-cloud-sdk && \
echo "âœ… Instalare completÄƒ! Verificare versiuni:" && \
bq version && \
gcloud version
```

#### **Pe macOS**
```bash
# Cu Homebrew
brew install --cask google-cloud-sdk

# SAU descarcÄƒ installer-ul manual:
# https://cloud.google.com/sdk/docs/install#mac

# VerificÄƒ instalarea
bq version
gcloud version
```

#### **Pe Windows**
```bash
# DescarcÄƒ installer-ul oficial:
# https://cloud.google.com/sdk/docs/install#windows

# RuleazÄƒ installer-ul GoogleCloudSDKInstaller.exe
# UrmeazÄƒ wizard-ul de instalare

# Deschide Command Prompt/PowerShell È™i verificÄƒ:
bq version
gcloud version
```

#### **Verificare instalare reuÈ™itÄƒ**
```bash
# Ar trebui sÄƒ vezi output similar cu:
# bq version 2.0.XX
# gcloud version XXX.X.X

# DacÄƒ vezi "command not found", adaugÄƒ Ã®n PATH:
# Linux/macOS:
export PATH=$PATH:$HOME/google-cloud-sdk/bin

# Windows (PowerShell):
# $env:Path += ";C:\Program Files (x86)\Google\Cloud SDK\google-cloud-sdk\bin"
```

### **2. Autentificare BigQuery**
```bash
# AutentificÄƒ-te cu contul Google Cloud
gcloud auth login

# SeteazÄƒ proiectul corect
gcloud config set project YOUR_PROJECT_ID

# VerificÄƒ cÄƒ ai acces la dataset
bq ls PanouControlUnitar
```

### **3. Backup (OPÈšIONAL, dar recomandat)**
```bash
# ExportÄƒ toate tabelele existente (safety backup)
mkdir -p ~/bigquery-backup
bq extract --destination_format=NEWLINE_DELIMITED_JSON \
  PanouControlUnitar.Proiecte \
  gs://YOUR_BUCKET/backup/Proiecte_*.json
# RepetÄƒ pentru toate tabelele importante
```

---

## ğŸ› ï¸ PASUL 1: CREAREA TABELELOR NOI (ZI 1-2)

### **1.1 Executare DDL Ã®n BigQuery Console**

**OpÈ›iunea A: BigQuery Console (recomandat pentru prima datÄƒ)**
1. Deschide [BigQuery Console](https://console.cloud.google.com/bigquery)
2. Click pe "Compose new query"
3. CopiazÄƒ conÈ›inutul din `scripts/bigquery-create-tables.sql`
4. SelecteazÄƒ blocuri de 5-10 tabele È™i ruleazÄƒ (nu toate deodatÄƒ, prea mare)
5. VerificÄƒ Ã®n stÃ¢nga cÄƒ apar tabelele `*_v2` Ã®n dataset

**OpÈ›iunea B: bq CLI (pentru automatizare)**
```bash
# RuleazÄƒ Ã®ntregul script DDL
bq query --use_legacy_sql=false < scripts/bigquery-create-tables.sql

# SAU ruleazÄƒ fiecare tabel individual (mai safe)
cat scripts/bigquery-create-tables.sql | grep -A 50 "CREATE TABLE" | head -52 | bq query --use_legacy_sql=false
```

### **1.2 Verificare tabele create**
```bash
# ListeazÄƒ toate tabelele v2
bq ls --max_results=100 PanouControlUnitar | grep "_v2"

# Ar trebui sÄƒ vezi 32 tabele noi cu sufixul _v2
# Exemplu output:
#   AnafEFactura_v2
#   AnafErrorLog_v2
#   ...
#   Utilizatori_v2
```

### **1.3 VerificÄƒ configuraÈ›ia partitioning**
```bash
# VerificÄƒ cÄƒ Proiecte_v2 are partitioning configurat
bq show --format=prettyjson PanouControlUnitar.Proiecte_v2 | grep -A 10 "timePartitioning"

# Output aÈ™teptat:
# "timePartitioning": {
#   "field": "Data_Start",
#   "type": "DAY"
# }
```

**âœ… CHECKPOINT 1:** DacÄƒ ai 32 tabele `_v2` Ã®n BigQuery È™i partitioning e configurat corect, continuÄƒ.

---

## ğŸ“¦ PASUL 2: COPIEREA DATELOR (ZI 3)

### **2.1 RECOMANDAT: Copiere direct Ã®n BigQuery Console (fÄƒrÄƒ autentificare CLI)**

**Avantaje**: Nu necesitÄƒ `gcloud auth login`, ruleazÄƒ instant Ã®n browser

1. **Deschide BigQuery Console**: https://console.cloud.google.com/bigquery
2. **Click "Compose new query"**
3. **CopiazÄƒ conÈ›inutul fiÈ™ierului** `scripts/bigquery-copy-data.sql`
4. **RuleazÄƒ fiecare bloc INSERT separat** (sau toate odatÄƒ, dacÄƒ vrei)
5. **RuleazÄƒ query-ul de verificare** de la final pentru a vedea count(*) pentru toate tabelele

**Exemplu query pentru o singurÄƒ copiazÄƒ:**
```sql
INSERT INTO `PanouControlUnitar.Proiecte_v2`
SELECT * FROM `PanouControlUnitar.Proiecte`;
```

**Query verificare finalÄƒ** (de la final fiÈ™ierului):
```sql
SELECT
  'Proiecte' as tabel,
  (SELECT COUNT(*) FROM `PanouControlUnitar.Proiecte`) as count_vechi,
  (SELECT COUNT(*) FROM `PanouControlUnitar.Proiecte_v2`) as count_nou
UNION ALL
SELECT 'Clienti',
  (SELECT COUNT(*) FROM `PanouControlUnitar.Clienti`),
  (SELECT COUNT(*) FROM `PanouControlUnitar.Clienti_v2`)
-- ... pentru toate tabelele
ORDER BY tabel;
```

Rezultatul ar trebui sÄƒ arate:
```
tabel           | count_vechi | count_nou
----------------|-------------|----------
Proiecte        | 125         | 125       âœ…
Clienti         | 48          | 48        âœ…
TimeTracking    | 1543        | 1543      âœ…
...
```

### **2.2 ALTERNATIV: Script bash (necesitÄƒ autentificare gcloud)**

**Doar dacÄƒ preferi CLI** (necesitÄƒ `gcloud auth login` mai Ã®ntÃ¢i):

```bash
# Autentificare Google Cloud (DOAR o datÄƒ)
gcloud auth login
gcloud config set project YOUR_PROJECT_ID

# RuleazÄƒ script
cd /home/teodor/PM1-2025-07-17/unitar-admin
./scripts/bigquery-copy-data.sh
```

### **2.2 Verificare manualÄƒ copiere (sample test)**
```bash
# ComparÄƒ count pentru cÃ¢teva tabele importante
bq query --use_legacy_sql=false "SELECT COUNT(*) FROM \`PanouControlUnitar.Proiecte\`"
bq query --use_legacy_sql=false "SELECT COUNT(*) FROM \`PanouControlUnitar.Proiecte_v2\`"
# Numerele trebuie sÄƒ fie IDENTICE

# Sample rows pentru verificare vizualÄƒ
bq query --use_legacy_sql=false "SELECT * FROM \`PanouControlUnitar.Proiecte\` ORDER BY RAND() LIMIT 3"
bq query --use_legacy_sql=false "SELECT * FROM \`PanouControlUnitar.Proiecte_v2\` ORDER BY RAND() LIMIT 3"
# Datele trebuie sÄƒ fie identice
```

**âœ… CHECKPOINT 2:** DacÄƒ toate tabelele v2 au acelaÈ™i count(*) ca tabelele vechi, continuÄƒ.

---

## ğŸ”§ PASUL 3: MODIFICARE API ROUTES (ZI 4-5)

### **3.1 CreeazÄƒ variabilÄƒ env pentru toggle tabele**

**EditeazÄƒ fiÈ™ierul `.env.local`** (creeazÄƒ-l dacÄƒ nu existÄƒ):

```bash
# Deschide fiÈ™ierul Ã®n editor:
nano .env.local
# SAU
code .env.local  # DacÄƒ foloseÈ™ti VS Code
```

**AdaugÄƒ linia:**
```
BIGQUERY_USE_V2_TABLES=true
```

**SalveazÄƒ È™i Ã®nchide fiÈ™ierul.**

**SAU comandÄƒ rapidÄƒ bash** (adaugÄƒ automat la final de fiÈ™ier):
```bash
echo "BIGQUERY_USE_V2_TABLES=true" >> .env.local
```

**Pentru Vercel Production** (MAI TÃ‚RZIU, la deployment):
- Ãn Vercel Dashboard â†’ Settings â†’ Environment Variables
- AdaugÄƒ: `BIGQUERY_USE_V2_TABLES` = `true`

### **3.2 ModificÄƒ API routes pentru tabele partiÈ›ionate**

**Pattern general pentru toate API-urile:**
```typescript
// Ãn fiecare API route care foloseÈ™te tabele partiÈ›ionate
const useV2 = process.env.BIGQUERY_USE_V2_TABLES === 'true';
const tableSuffix = useV2 ? '_v2' : '';

// Exemplu pentru /api/rapoarte/proiecte/route.ts
const query = `
  SELECT * FROM \`PanouControlUnitar.Proiecte${tableSuffix}\`
  WHERE Data_Start >= @data_start_min
    AND Data_Start <= @data_start_max
    AND Status = @status
  ORDER BY Data_Start DESC
  LIMIT @limit
`;

// AdaugÄƒ parametri pentru partition filter
const options = {
  query,
  params: {
    data_start_min: dataStartMin || '2023-01-01',  // Default ultimii 2 ani
    data_start_max: dataStartMax || new Date().toISOString().split('T')[0],
    status: status || 'Activ',
    limit: parseInt(limit) || 100
  }
};
```

### **3.3 API-uri de modificat (ORDINEA PRIORITÄ‚ÈšII)**

**HIGH PRIORITY (8 fiÈ™iere - zilele 4-5):**
1. `/api/rapoarte/proiecte/route.ts` - PARTITION BY Data_Start
2. `/api/rapoarte/facturi/route.ts` - PARTITION BY data_factura
3. `/api/analytics/time-tracking/route.ts` - PARTITION BY data_lucru
4. `/api/rapoarte/contracte/route.ts` - PARTITION BY Data_Semnare
5. `/api/tranzactii/dashboard/route.ts` - PARTITION BY data_procesare
6. `/api/rapoarte/clienti/route.ts` - CLUSTER BY cui
7. `/api/planificator/items/route.ts` - PARTITION BY DATE(data_adaugare)
8. `/api/user/timetracking/route.ts` - PARTITION BY data_lucru

**Exemplu concret modificare (vezi `BIGQUERY-MIGRATION-PLAN.md` pentru detalii complete)**

### **3.4 Testare Ã®n localhost dupÄƒ fiecare API modificat**
```bash
# Start localhost cu tabele v2
BIGQUERY_USE_V2_TABLES=true npm run dev

# TesteazÄƒ Ã®n browser:
# - http://localhost:3000/admin/rapoarte/proiecte
# - http://localhost:3000/admin/rapoarte/facturi
# - http://localhost:3000/time-tracking

# VerificÄƒ Ã®n Network tab (Chrome DevTools):
# - Status 200 OK
# - Date returnate sunt corecte
# - Console fÄƒrÄƒ erori
```

**âœ… CHECKPOINT 3:** DacÄƒ toate cele 8 API-uri HIGH PRIORITY funcÈ›ioneazÄƒ Ã®n localhost, continuÄƒ.

---

## ğŸ§ª PASUL 4: TESTARE COMPLETÄ‚ LOCALHOST (ZI 6)

### **4.1 End-to-end testing**
```bash
# Cu BIGQUERY_USE_V2_TABLES=true Ã®n .env.local
npm run dev

# TesteazÄƒ toate flow-urile aplicaÈ›iei:
âœ… Login È™i autentificare
âœ… Dashboard executiv (/admin)
âœ… Management proiecte (creare, editare, È™tergere)
âœ… Time tracking (start timer, stop, istoric)
âœ… Facturi (generare, listare)
âœ… TranzacÈ›ii bancare (import CSV, matching)
âœ… Planificator personal (adÄƒugare items, reordonnare)
```

### **4.2 Performance testing**
```typescript
// AdaugÄƒ Ã®n fiecare API route testat (temporar, pentru debug):
console.time('BigQuery Query');
const [rows] = await bigquery.query(options);
console.timeEnd('BigQuery Query');

console.log(`ğŸ“Š Rows returned: ${rows.length}`);
console.log(`ğŸ’¾ Bytes processed: ${(rows.length * 2000).toLocaleString()} bytes (estimate)`);
```

**Output aÈ™teptat:**
```
BigQuery Query: 245ms  (vs ~800ms fÄƒrÄƒ partitioning)
ğŸ“Š Rows returned: 48
ğŸ’¾ Bytes processed: 96,000 bytes (vs ~2MB fÄƒrÄƒ partitioning)
```

### **4.3 ComparaÈ›ie v1 vs v2**
```bash
# TesteazÄƒ acelaÈ™i query cu È™i fÄƒrÄƒ v2
# Toggle Ã®ntre BIGQUERY_USE_V2_TABLES=true È™i false

# ComparÄƒ rezultatele:
# - Datele returnate trebuie IDENTICE
# - Timpul query trebuie MAI MIC cu v2 (dacÄƒ ai partitioning filter)
# - Bytes scanned trebuie MAI PUÈšIN cu v2 (vezi Ã®n BigQuery Console â†’ Query history)
```

**âœ… CHECKPOINT 4:** DacÄƒ aplicaÈ›ia funcÈ›ioneazÄƒ 100% identic cu v1, dar mai rapid, continuÄƒ.

---

## ğŸš€ PASUL 5: DEPLOY PRODUCTION (ZI 7)

### **5.1 Commit toate modificÄƒrile**
```bash
git add .
git commit -m "ğŸš€ BigQuery optimization: partitioning + clustering (v2 tables)

- Created 32 optimized tables with partitioning/clustering
- Modified 8 HIGH PRIORITY API routes
- Added BIGQUERY_USE_V2_TABLES env toggle
- Estimated 90-95% cost reduction on BigQuery queries
- All functionality preserved, performance improved"

git push origin main
```

### **5.2 Deploy Vercel cu tabele v2**
```bash
# Ãn Vercel Dashboard â†’ Settings â†’ Environment Variables
# AdaugÄƒ:
BIGQUERY_USE_V2_TABLES = true

# Redeploy aplicaÈ›ia
vercel --prod
```

### **5.3 Monitorizare 24h post-deploy**
```bash
# VerificÄƒ Vercel Logs pentru erori
# https://vercel.com/your-project/logs

# VerificÄƒ BigQuery Query History pentru costuri
# https://console.cloud.google.com/bigquery?project=YOUR_PROJECT&page=queries

# FiltreazÄƒ dupÄƒ ultimele 24h È™i comparÄƒ:
# - Bytes scanned per query (ar trebui 85-95% mai puÈ›in)
# - Query duration (ar trebui 50-70% mai rapid)
```

**âœ… CHECKPOINT 5:** DacÄƒ totul merge OK 24h fÄƒrÄƒ erori critice, continuÄƒ cu È™tergerea tabelelor vechi.

---

## ğŸ—‘ï¸ PASUL 6: CLEANUP TABELE VECHI (DupÄƒ 1 sÄƒptÄƒmÃ¢nÄƒ rulare OK)

### **6.1 ATENÈšIE: IRREVERSIBIL - verificÄƒ checklist final**
```
âœ… Toate tabelele v2 au acelaÈ™i count(*) ca tabelele vechi
âœ… API-urile returneazÄƒ date identice cu v1 vs v2
âœ… Localhost funcÈ›ioneazÄƒ 100% cu v2
âœ… Production Vercel ruleazÄƒ OK 7 zile cu v2
âœ… Zero erori critice Ã®n Vercel logs
âœ… Costuri BigQuery vizibil scÄƒzute Ã®n Google Cloud Console
âœ… (OPÈšIONAL) Backup export fÄƒcut pentru tabele vechi
```

### **6.2 È˜tergere tabele vechi**
```bash
# Script automat È™tergere tabele vechi (32 tabele)
# ATENÈšIE: IRREVERSIBIL!

TABLES=(
  "AnafEFactura" "AnafErrorLog" "AnafNotificationLog" "AnafTokens"
  "AnexeContract" "Clienti" "Contracte" "CursuriValutare"
  "EtapeContract" "EtapeFacturi" "FacturiGenerate" "FacturiPrimite"
  "PlanificatorPersonal" "ProcesVerbale" "Produse"
  "ProiectComentarii" "Proiecte" "ProiecteCheltuieli"
  "ProiecteResponsabili" "Sarcini" "SarciniResponsabili"
  "SesiuniLucru" "Subcontractanti" "Subproiecte"
  "SubproiecteResponsabili" "TimeTracking" "TranzactiiAccounts"
  "TranzactiiBancare" "TranzactiiMatching" "TranzactiiStats"
  "TranzactiiSyncLogs" "Utilizatori"
)

for table in "${TABLES[@]}"; do
  echo "ğŸ—‘ï¸  È˜terg tabelul: $table"
  bq rm -f PanouControlUnitar.$table
done

echo "âœ… Tabele vechi È™terse!"
```

### **6.3 Redenumire tabele v2 â†’ nume original**
```bash
# Script redenumire automat
for table in "${TABLES[@]}"; do
  echo "ğŸ“ Redenumesc: ${table}_v2 â†’ $table"

  # Copy v2 â†’ original name
  bq cp -f PanouControlUnitar.${table}_v2 PanouControlUnitar.$table

  # Delete v2
  bq rm -f PanouControlUnitar.${table}_v2
done

echo "âœ… Redenumire completÄƒ!"
```

### **6.4 EliminÄƒ env variable toggle**
```bash
# Ãn .env.local È™i Vercel env variables
# È˜terge: BIGQUERY_USE_V2_TABLES=true

# EliminÄƒ din cod verificÄƒrile useV2 È™i tableSuffix
# PÄƒstreazÄƒ doar versiunea optimizatÄƒ finalÄƒ
```

**âœ… CHECKPOINT 6:** Migrare 100% completÄƒ! Tabelele optimizate sunt acum tabelele principale.

---

## ğŸ“Š VERIFICARE ECONOMII FINALE

### **Ãnainte de migrare:**
```bash
# Ãn BigQuery Console â†’ Query History â†’ filtru ultimele 30 zile
# Suma "Bytes processed" pentru toate query-urile
# Exemplu: 500 GB procesate/lunÄƒ
# Cost: 500 GB Ã— $5/TB = $2.50/lunÄƒ
```

### **DupÄƒ migrare:**
```bash
# VerificÄƒ aceeaÈ™i perioadÄƒ cu tabele v2
# Exemplu: 50 GB procesate/lunÄƒ (90% reducere)
# Cost: 50 GB Ã— $5/TB = $0.25/lunÄƒ
# ECONOMIE: $2.25/lunÄƒ (~$27/an)
```

---

## ğŸš¨ ROLLBACK PLAN (DacÄƒ ceva nu merge)

### **Ãn orice moment ÃNAINTE de È™tergerea tabelelor vechi:**
```bash
# 1. OpreÈ™te folosirea tabelelor v2
# Ãn .env.local È™i Vercel:
BIGQUERY_USE_V2_TABLES=false

# 2. Redeploy aplicaÈ›ia
vercel --prod

# 3. AplicaÈ›ia revine INSTANT la tabelele vechi
# Zero downtime, zero pierdere date
```

### **DupÄƒ È™tergerea tabelelor vechi (dacÄƒ ai backup):**
```bash
# Restaurare din backup GCS
bq load --source_format=NEWLINE_DELIMITED_JSON \
  PanouControlUnitar.Proiecte \
  gs://YOUR_BUCKET/backup/Proiecte_*.json

# RepetÄƒ pentru toate tabelele din backup
```

---

## ğŸ“š RESURSE

- **Plan complet**: `/BIGQUERY-MIGRATION-PLAN.md`
- **DDL tabele**: `/scripts/bigquery-create-tables.sql`
- **Script copiere**: `/scripts/bigquery-copy-data.sh`
- **BigQuery Docs**: https://cloud.google.com/bigquery/docs/partitioned-tables

---

## âœ… CHECKLIST FINAL SUCCESS

```
âœ… 32 tabele v2 create Ã®n BigQuery
âœ… Toate datele copiate cu success (count identic)
âœ… 8 API-uri HIGH PRIORITY modificate È™i testate
âœ… Localhost funcÈ›ioneazÄƒ 100% cu v2
âœ… Production Vercel ruleazÄƒ OK cu v2
âœ… Costuri BigQuery reduse cu 90-95%
âœ… Tabele vechi È™terse, v2 redenumite â†’ original
âœ… AplicaÈ›ie production foloseÈ™te tabele optimizate
âœ… ECONOMIE ANUALÄ‚: ~$200-300
```

**SUCCES! ğŸ‰**

---

**Data creare**: 01.10.2025
**Ultima actualizare**: 01.10.2025
**Status**: ğŸŸ¢ GATA PENTRU IMPLEMENTARE
